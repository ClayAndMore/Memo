
---
title: "容器网络.md"
date: 2019-09-29 17:53:13 +0800
lastmod: 2020-03-23 22:54:01 +0800
draft: false
tags: [""]
categories: [""]
author: "Claymore"

---
Tags:[Docker]

## 容器网络

网络 虚拟化

### 网络虚拟设备

linux是通过网络设备区操作和使用网卡的，系统装了一个网卡后会为其生成一个网络设备实例，如eth0.

linux也支持创建出虚拟化的设备，主要记录Veth 和 Bridge

`ip` 命令管理的功能很多， 和 network namespace 有关的操作都是在子命令 `ip netns` 下进行的

#### ip netns

ip命令的netns可以让一台机器上模拟多个网络设备，是网络虚拟化的重要组成，将不同类型的网络应用隔离。

需要内核支持，不然ip 没有netns命令

一个net namespace有自己独立的路由表，iptables策略，设备管理。说来说去，它就是用来隔离的。比如将eth0加入了netns 1，那么netns 2中的应用程序就找不到eth0了。netns 1中的iptables策略，不会去影响netns 2中的iptables策略。

用法：

```
[root@monitor ~]# ip netns help list
Usage: ip netns list
       ip netns add NAME
       ip netns set NAME NETNSID
       ip [-all] netns delete [NAME]
       ip netns identify [PID]
       ip netns pids NAME
       ip [-all] netns exec [NAME] cmd ...
       ip netns monitor
       ip netns list-id
```



* 打开内核的网络转发功能

  ```
  [root@localhost ~]# vim /etc/sysctl.conf 
  [root@localhost ~]# sysctl -p
  net.ipv4.ip_forward = 1
  ```

* 添加两个namespace

  ```
  [root@monitor ~]# ip netns add r1
  [root@monitor ~]# ip netns add r2
  [root@monitor ~]# ip netns list
  r2
  r1
  ```





#### Veth

有了不同 network namespace 之后，也就有了网络的隔离，但是如果它们之间没有办法通信，也没有实际用处。要把两个网络连接起来，linux 提供了 `veth pair`

Veth pair 是成对出现的虚拟网络设备， 发送到Veth的一端虚拟设备请求会从另一端的虚拟设备发出。

veth pair可以理解为使用网线连接好的两个接口，把两个端口放到两个namespace中，那么这两个namespace就能打通。

创建两个网络namespace:

```bash
root@:# ip netns add ns1
root@:# ip netns list
ns1
root@# ip netns add ns2
```

创建一对veth

```bash
root@# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 52:54:00:86:04:2a brd ff:ff:ff:ff:ff:ff
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff
5: veth9ea5fdd: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
    link/ether 26:dc:7d:94:97:dd brd ff:ff:ff:ff:ff:ff
7: veth5faf20e: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
    link/ether 62:a2:16:40:72:6e brd ff:ff:ff:ff:ff:ff

root@# ip link add veth0 type veth peer name veth1
root@# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 52:54:00:86:04:2a brd ff:ff:ff:ff:ff:ff
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff
5: veth9ea5fdd: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
    link/ether 26:dc:7d:94:97:dd brd ff:ff:ff:ff:ff:ff
7: veth5faf20e: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
    link/ether 62:a2:16:40:72:6e brd ff:ff:ff:ff:ff:ff
8: veth1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether ba:5f:b9:c5:c3:28 brd ff:ff:ff:ff:ff:ff
9: veth0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether c6:85:e2:28:fc:4d brd ff:ff:ff:ff:ff:ff
```
分别将两个veth 移到Namespace中。
```bash
root@:# ip link set veth0 netns ns1
root@:u# ip link set veth1 netns ns2
# 去 ns1 的namespace 中查看网络
root@:# ip netns exec ns1 ip link
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
9: veth0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether c6:85:e2:28:fc:4d brd ff:ff:ff:ff:ff:ff
```
可以看到，除了loopback的设备，有个veth0, 当请求 发送到这个虚拟网络时， 都会从另一个网络Namespace 的网络接口中出来。

配置每个veth 的网络地址和路由
```bash
root@:# ip netns exec ns1 ifconfig veth0 172.18.0.2/24 up
root@:# ip netns exec ns2 ifconfig veth1 172.18.0.3/24 up
root@:# ip netns exec ns1 route add default dev veth0
root@:/home/ubuntu# ip netns exec ns2 route add default dev veth1
```
此时：

```
       ns1               ns2
+-------------+      +----------------+
|      veth0  |      |  veth1         |
|             |      |                |
|172.18.0.2/24|      |172.18.0.3/24   |
+-------------+      +----------------+
```

通过veth一端的包，另一端一定能够接收到。

```basj
root@:/home/ubuntu# ip netns exec ns1 ping -c 1 172.18.0.3
PING 172.18.0.3 (172.18.0.3) 56(84) bytes of data.
64 bytes from 172.18.0.3: icmp_seq=1 ttl=64 time=0.050 ms

--- 172.18.0.3 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.050/0.050/0.050/0.000 ms

```



#### Linux Bridge

虽然 veth pair 可以实现两个 network namespace 之间的通信，但是当多个 namespace 需要通信的时候，就无能为力了。
讲到多个网络设备通信，我们首先想到的交换机和路由器。因为这里要考虑的只是同个网络，所以只用到交换机的功能。linux 当然也提供了虚拟交换机的功能，

Bridge 虚拟设备是用来桥接的网络设备， 它相当于现实世界中的交换机，可以连接不同的网络设备。

当请求到达Bridge设备时， 可以通过报文中的Mac地址进行广播或转发。

eg: 通过创建一个Bridge 设备来连接Namespace 中的网络设备和宿主机上的网络。

```bash
# 创建 veth 设备并将一端移入 Namespace
ip netns add ns1
ip link add veth0 type veth peer name veth1
ip link set veth1 netns ns1

# 创建网桥
brctl addbr br0  
# 挂载网络设备
brctl addif br0 eth0  # 注意这里，因为这里我又重装了系统
brctl addif br0 veth0
```

示意图：

```
主机              __________
                 |    ns1  |
                 |__veth1__|
                      |
____________________veth0____
             br0             |
__eth0_______________________|
```

参考：https://cizixs.com/2017/02/10/network-virtualization-network-namespace/



#### 路由表

路由表是Linux 内核的一个模块， 通过定义路由表来决定在某个网络Namespace中包的流向， 从而定义请求会到哪个网络设备上。

继续用上面的例子演示路由表的功能：

```bash
# 启动虚拟网络设备，并设置它在Net Namespace 中的IP地址。
pass 
```





### iptables

Iptables 定义了 一套链式处理结构， 在网络包传输的各个节点可以使用不同的策略对包进行加工，传送或丢弃。

在容器虚拟化的技术中，经常会用到两种策略MASQUERADE 和 DNAT 用于容器和宿主机外部的网络通信。



#### MASQUERADE

Iptables 中的MASQUERADE策略可以将请求包中的源地址转换成一个网络设备的地址。

如容器内的地址可以路由到网桥，但是到达宿主机外部之后，是不知道如何路由到容器内的这个地址的。

所以如果请求外部地址的话，需要先通过MASQUERADE 将 容器内ip转换成宿主机出口网卡的ip:

```bash
# 打开IP 转发
sysctl -w net.ipv4.conf.all.forwarding=1

# 对namespace 中发出的包添加网络地址转换
iptables -t nat -A POSTROUTING -s 172.18.0.0/24 -o eth0 -j MASQUERADE
```

这样就可以在Namespace 中访问宿主机外的网络了。



#### DNAT

和上述类似，也是做网络地址的转换，不过是要更换目标地址。

比如外部应用想要调取容器内怎么办呢

```
# 将宿主机上的80端口请求转发到Namespace的IP上
iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.18.0.2:80
```

这样就可以把宿主机上的80端口的TCP请求转发到Namespace 中的地址172.18.0.2:80，从而实现外部的应用调用。



### demo 解读bridge模式下的通信

在bridge模式下，连在同一网桥上的容器可以相互通信（若出于安全考虑，也可以禁止它们之间通信，方法是在DOCKER_OPTS变量中设置–icc=false，这样只有使用–link才能使两个容器通信）。

Docker可以开启容器间通信（意味着默认配置–icc=true），也就是说，宿主机上的所有容器可以不受任何限制地相互通信，这可能导致拒绝服务攻击。进一步地，Docker可以通过–ip_forward和–iptables两个选项控制容器间、容器和外部世界的通信。

容器也可以与外部通信，我们看一下主机上的Iptable规则，可以看到这么一条

`-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE`

这条规则会将源地址为172.17.0.0/16的包（也就是从Docker容器产生的包），并且不是从docker0网卡发出的，进行源地址转换，转换成主机网卡的地址。这么说可能不太好理解，举一个例子说明一下。假设主机有一块网卡为eth0，IP地址为10.10.101.105/24，网关为10.10.101.254。从主机上一个IP为172.17.0.1/16的容器中ping百度（180.76.3.151）。IP包首先从容器发往自己的默认网关docker0，包到达docker0后，也就到达了主机上。然后会查询主机的路由表，发现包应该从主机的eth0发往主机的网关10.10.105.254/24。接着包会转发给eth0，并从eth0发出去（主机的ip_forward转发应该已经打开）。这时候，上面的Iptable规则就会起作用，对包做SNAT转换，将源地址换为eth0的地址。这样，在外界看来，这个包就是从10.10.101.105上发出来的，Docker容器对外是不可见的。

那么，外面的机器是如何访问Docker容器的服务呢？我们首先用下面命令创建一个含有web应用的容器，将容器的80端口映射到主机的80端口。

`docker run --name=nginx_bridge --net=bridge -p 80:80 -d nginx`

然后查看Iptable规则的变化，发现多了这样一条规则：

`-A DOCKER ! -i docker0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80`

此条规则就是对主机eth0收到的目的端口为80的tcp流量进行DNAT转换，将流量发往172.17.0.2:80，也就是我们上面创建的Docker容器。所以，外界只需访问10.10.101.105:80就可以访问到容器中的服务。

除此之外，我们还可以自定义Docker使用的IP地址、DNS等信息，甚至使用自己定义的网桥，但是其工作方式还是一样的。

