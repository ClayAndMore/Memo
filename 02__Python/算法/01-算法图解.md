tags:[算法]



## 算法简介

### 二分查找

情景：在电话簿中找以K开头的姓名； 在数据库中找到某账户，凡是查找问题可以用二分法来解决。

前提：输入是一个**有序列表**

对于包含n个元素的列表，用二分查找最多需要log2N步。

```python
def binary(sort_list, num):
    """ 二分法
    给定一有序数组，和待找数字
    返回找到下标和次数，如果没有找到则返回None
    """
    low = 0
    high = len(sort_list)-1

    i = 0 # 查找次数
    while low <= high:
        i += 1
        middle = (low + high) // 2
        n =sort_list[middle]
        if n < num:
            # 开始时用的low = n, 没有搞清逻辑。个人觉得也可不增减一
            low = middle + 1
        elif n > num:
            high = middle - 1
        else:
            return middle, i
    return None, i

print binary(list(range(100)), 7)
#print binary([1,3,5,7,9],3)
```



### 数组和链表

|      | 数组 | 链表 |
| ---- | ---- | ---- |
| 读取 | O(1) | O(n) |
| 插入 | O(n) | O(1) |
| 删除 | O(n) | O(1) |

数组的空间是连续的，比如 c 语言 等声明数组要先开辟空间，当插入时空间不够，所有元素还要再移动。

链表的空间不是连续的，指向了下个地址。



### 选择排序

```python
# coding:utf-8
def selector_sort(arr):
    def find_small(arr_temp):
        """
        找到最小的元素
        """
        small = arr_temp[0]
        index = 0
        for x, y in enumerate(arr_temp):
            if y < small:
                small = y
                index = x
        return index
    new_arr = []
    for i in range(len(arr)):
        s = find_small(arr)
        new_arr.append(arr.pop(s))
    return new_arr


print selector_sort([4,2,23,55,3,5])
```

就是从头到尾固定一个数，找剩下最小的，随着查找，剩下的数剩的越少。

时间复杂度O(n*n)， 为什么剩下的数越来越少，仍然是n方的时间复杂度？

检查的元素为n, n-1,n-2…1, 评价检查的元素数为1/2 n, 所以：O(1/2n * n ) 去掉常数 = O(n*n)



### 递归和栈

递归， 记住两个条件：

* 基线条件，退出时的逻辑
* 递归条件， 继续递归的条件



### 栈

调用栈：用于存储多个函数的变量。

栈比较占内存。



### 分而治之 D&C

递归式问题解决方法。

工作原理：

1. 找出简单的基线条件；
2. 确定如何缩小问题的规模，使其符合基线条件。



### 快速排序

快速排序是一种常用的排序算法，比选择排序快得多。

例如，C语言标准库中的函数qsort 实现的就是快速排序，快速排序也使用了D&C.

基线条件： 

```
----- [] 空数组
|
----- [1] 值包含一个元素的数组
```

缩小问题规模

* 选定基准值，大于基准值的放在右侧，小于的放在左侧再形成两个数组

* 对每个数组再重复上述步骤



代码：

```python
def quick_sort(arr):
    if len(arr) < 2: return arr
    flag = arr[0]
    left = []
    right = []
    for x in arr[1:]: # 注意这里的1：
        if x < flag:
            left.append(x)
        else:
            right.append(x)
    return quick_sort(left) + [flag] + quick_sort(right)

print quick_sort([3, 2, 4, 6, 5])
```

最好情况 O(nlogn), 最坏： O(n*n)

平均复杂度： O(nlogn), **因为规定最佳情况也是平均情况**



### 散列表

散列表比较复杂，它使用散列函数确定元素的存储位置。

应用实例： dns 解析： google.com -> 74.125.239.133



#### 散列方法

我们可以在元素存储位置与其关键码之间建立一个确定的对应函数关系Hash()， 使得每个关键码与结构中一个**唯一**的**存储位置**相对应，即Address ＝ Hash(key)

有几种实现方式：

1. 除留余数法

   设散列表中允许地址数为m，取一个不大于 m，但最接近于或等于 m 的质数 p 作为除数，用以下函数把关键码转换成散列地址：

   `hash (key) = key % p`         p 为 <= m的质数

   其中，“%”是整数除法取余的运算，要求这时的质数 p 不是接近 2 的幂。

   举例: 有一个关键码 key = 962148，散列表大小 m = 25，即 HT[25]。取质数 p = 23。散列函数 hash(key) = key % p。则散列地址为

   `hash(962148) = 962148 % 23 = 12`

   可按计算出的地址存放记录。注意,  使用散列函数计算出的地址范围是 0 到 22，而 23、24 这几个地址实际上不能用散列函数计算出来，只能在处理冲突时达到这些地址

2. 直接定址法：取关键字的线性函数值作为哈希地址。

3. 数字分析法：取关键字的中的若干位作为哈希地址。

4.  平方取中法：取关键字平方后的中间几位作为哈希地址。

5. 折叠法：将关键字分割成位数相同的几部分（最后一部分可以不同），然后取这几部分的叠加和作为哈希地址。

这些仅了解，用的比较多的还是余数法。



#### 冲突

不可能有这样的函数，能填满所有位置而不重复，重复时我们称之为冲突。

如果两个键都映射到了一个位置，简单的解决办法是在冲突的地方建立链表(**链地址法**)：

```
+------+
| 039  |      +-------+-------+----+     +-------------+
|    +--------+ apple |  1.45 |  |------>+  banana|2.43|
+------+      +-------+-------+----+     +-------------+
|      |
|      |
|      |
+------+
|      |
|      |
+------+
```

实际的哈希表实现中，使用最多的是链地址法

其他方法的补充：

**开放地址法**

当发生地址冲突时，按照某种方法继续探测哈希表中的其他存储单元，直到找到空位置为止。这个过程可用下式描述： 
`Hi(key) = ( H( key ) + di ) mod m ( i = 1,2,……, m – 1)) `

* H ( key ) 为关键字 key 的直接哈希地址， 
* m 为哈希表的长度，
* di 为每次再探测时的地址增量。 

增量 d 可以有不同的取法，并根据其取法有不同的称呼： 
（ 1 ） d i ＝ 1 ， 2 ， 3 ， …… **线性探测再散列**； 
（ 2 ） d i ＝ 1^2 ，－ 1^2 ， 2^2 ，－ 2^2 ， k^2， -k^2…… **二次探测再散列**； 
（ 3 ） d i ＝ 伪随机序列 伪随机再散列； 

Eg： 设有哈希函数 H ( key ) = key mod 7 ，哈希表的地址空间为 0 ～ 6 ，对关键字序列（ 32 ， 13 ， 49 ， 55 ， 22 ， 38 ， 21 ）按线性探测再散列和二次探测再散列的方法分别构造哈希表：

```
（ 1 ）线性探测再散列： 
32 ％ 7 = 4 ； 13 ％ 7 = 6 ； 49 ％ 7 = 0 ； 
55 ％ 7 = 6 发生冲突，下一个存储地址（ 6 ＋ 1 ）％ 7 ＝ 0 ，仍然发生冲突，再下一个存储地址：（ 6 ＋ 2 ）％ 7 ＝ 1 未发生冲突，可以存入。 
22 ％ 7 ＝ 1 发生冲突，下一个存储地址是：（ 1 ＋ 1 ）％ 7 ＝ 2 未发生冲突； 
38 ％ 7 ＝ 3 ； 
21 ％ 7 ＝ 0 发生冲突，按照上面方法继续探测直至空间 5 ，不发生冲突，所得到的哈希表对应存储位置： 
下标： 0 1 2 3 4 5 6 
49 55 22 38 32 21 13 
（ 2 ）二次探测再散列： 
下标： 0 1 2 3 4 5 6 
49 22 21 38 32 55 13 
   注意：对于利用开放地址法处理冲突所产生的哈希表中删除一个元素时需要谨慎，不能直接地删除，因为这样将会截断其他具有相同哈希地址的元素的查找地址，所以，通常采用设定一个特殊的标志以示该元素已被删除。
```



#### 填装因子

填装因子 = 散列包含的元素数 / 位置总数

如申请了 10 个位置，有4个位置被填上了，则填装因子为0.4

填装因子度量的是散列表中有多少位置是空的。

如果因子大于1，那么就要扩展你的位置，其实一个不错的经验是大于0.7 时就扩展，这样发生的冲突也低。



### 广度优先搜索(BFS)

Breadth-First Search

广度优先搜索是一种基于图（树也可以）的查找算法，可以解决两类问题：

* 第一类问题， 从节点A出发， 有前往节点B的路径么
* 第二类问题，从节点A出发，前往节点B的哪条路径最短

问题引入， 假设你是种芒果的，你需要从你的朋友中找到一个芒果经销商(称为X),这个过程则对于上方一类问题。

如果朋友中没有就要从朋友的朋友（二度朋友）中找， 二类问题。

在python 中表达图的形式：

e.g.: 朋友关系图

```python
a  <---+
       |
       |                   +--> f
       +                   +
         b <-------+  +--> e +--->  j
         +         +  |
         |        you +
         v         +
         c <-----+ v
                  d
==============
graph = {}
graph[you] = [b, e, d]
graph[b] = [a, c] # c 是两个人的朋友，所以添加两次。
graph[d] = [c]
graph[e] = [f,j]
graph[a] = []
graph[f] = []
graph[j] = []
graph[c] = [] # 这里是有向图，我们认为关系是单向的。
```

为此我们要申请一个**队列**作为名单， 从名单头部开始找，当然你的一度朋友会先写入名单。

```python
from collection import deque
search_queue = deque[] # 创建一个队列
search_queue = graph["you"] # 将你的邻居加入到这个搜索队列中
searched = [] # 搜索过的加入这里，不然可能造成无限循环

while search_queue: # 只要队列不为空
    person = search_queue.popleft() # 取出其中的第一人
    if persion not in searched and persion_is_seller(persion)： # 检查这个人是否是芒果销售商
    	  return True
    else:
        search_queue += graph[persion] # 如果不是X , 就将这个人的朋友加入搜索队列
return False
```

从图的结构上说，如果任务A 依赖任务B, A必须在B后执行，那么这种结构被称为拓扑排序，可以根据它创建一个有序列表。



### 深度优先搜索(DFS)

Depth-First Search

深度优先直白讲是一种一条道走到黑，撞了南墙再回头的算法。所以其整个搜索空间可以表示为一个多叉树。

其可以用**递归和栈**来分别实现。

eg: 

![](http://claymore.wang:5000/uploads/big/c8f786f583825d8295b24d3d35c6ecc0.png)



栈实现：

```python
graph = {'A': set(['B', 'C']),
         'B': set(['A', 'D', 'E']),
         'C': set(['A', 'F']),
         'D': set(['B']),
         'E': set(['B', 'F']),
         'F': set(['C', 'E'])}

def dfs(graph, start):
    visited, stack = set(), [start]
    while stack:
        vertex = stack.pop()
        if vertex not in visited:
            visited.add(vertex)
            stack.extend(graph[vertex] - visited)
    return visited

print dfs(graph, 'A')
```

相关链接：https://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/



### 贪婪算法

贪婪算法是每一步都取最优解，但是整体下来不一定是最优解，如果需要能够大致解决问题的方法，使用贪婪算法的结果能与最优解相近。并且它速度比较快。

有的问题精确算法的时间不一定能有贪婪快。



#### NP问题

**引入P类问题的概念：如果一个问题可以找到一个能在多项式的时间里解决它的算法，那么这个问题就属于P问题**。P是英文单词多项式的第一个字母。

**NP问题不是非P类问题（关键 N不是not的意思）。NP问题是指可以在多项式的时间里验证一个解的问题**。NP问题的另一个定义是，可以在多项式的时间里猜出一个解的问题

注意定义，这里是验证。NP类问题，我用个人的俗话理解就是，不知道这个问题是不是存在多项式时间内的算法，所以叫non-deterministic非确定性，但是我们可以在多项式时间内验证并得出这个问题的一个正确解。举个例子，

著名的NP类问题：旅行家推销问题(TSP)。即有一个推销员，要到n个城市推销商品，他要找出一个包含所有n个城市的环路，这个环路路径小于a。我们知道这个问题如果单纯的用枚举法来列举的话会有(n-1)! 种，已经不是多项式时间的算法了，(注：阶乘算法比多项式的复杂)。那怎么办呢？我们可以用猜的，假设我人品好，猜几次就猜中了一条小于长度a的路径，我画画画画，好的，我得到了一条路径小于a的环路，问题解决了，皆大欢喜。可是，我不可能每次都猜的那么准，也许我要猜完所有种呢？所以我们说，这是一个NP类问题。也就是，我们能在多项式的时间内验证并得出问题的正确解，可是我们却不知道该问题是否存在一个多项式时间的算法，每次都能解决他(注意，这里是不知道，不是不存在)。

我们可以再用集合的观点来说明。如果把所有P类问题归为一个集合P中，把所有 NP问题划进另一个集合NP中，那么，显然有P属于NP。

现在，所有对NP问题的研究都集中在一个问题上，即究竟是否有P=NP？通常所谓的“NP问题”，其实就一句话：证明或推翻P=NP。**这才是NP问题，而不是指这个问题不能再多项式时间内求解**

**简单地说，存在多项式时间的算法的一类问题，称之为P类问题；至今没有找到多项式算法解的一类问题，称之为NP类问题。**



然后扯个题外话，为什么我们要研究这个？因为计算机处理的输入常常不是那么几十个几千个那么一点点，想象一下，当计算机处理的数据达到100万个的时候，时间复杂度为o(n^2)和o(e^n)的算法，所需的运行次数简直是天壤之别，o(e^n)指数级的可能运行好几天都没法完成任务，所以我们才要研究一个问题是否存在多项式时间算法。而我们也只在乎一个问题是否存在多项式算法，因为一个时间复杂度比多项式算法还要复杂的算法研究起来是没有任何实际意义的。



#### 识别np问题

下面是可能是np问题的情况：

* 涉及所有组合的通常是NP问题
* 不能将问题分成小问题，必须考虑各种情况。
* 问题涉及序列(如旅行商问题中的城市序列)，最短路径等问题。

* 设计广播合集(选几种广播覆盖全美国各个州)



如果遇到NP问题，用相近解法即可获得不错的答案，如贪婪法。