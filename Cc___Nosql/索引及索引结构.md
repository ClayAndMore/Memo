基于hash：如redis/memcached，随机读写O(1)，不支持顺序读写；

磁盘查找树B/B+树：mysql，B+树在大量的随机写时出现大量磁盘I/O，速度变慢

lsmtree：leveldb、rocksdb，先存内存后落盘，落盘的文件按key有序划分，会定期进行排序合并，合并后数据写入下一层Level。每次写都是写内存，


### 

索引：https://segmentfault.com/a/1190000018719035



### 索引

索引（index）本质上是一个存储了可提高检索效率的数据结构的文件。有了它使我们而不需要遍历数据库中的每一行。注意它只是个文件，也要趟在硬盘上的，使用时会加载到内存。

典型的索引譬如在内存中维护一个二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在 O(log2n)的复杂度内获取到相应数据。

一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。

**这样的话，索引查找过程中就要产生磁盘 I/O 消耗，相对于内存存取，I/O 存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘 I/O 操作次数的渐进复杂度。**

换句话说，索引的结构组织要尽量减少查找过程中磁盘 I/O 的存取次数。



### 哈希索引

![](G:\picture\blog\databases\hash_index.jpg)

哈希索引即是基于哈希技术，如上图所示，我们将一系列的最终的键值通过哈希函数转化为存储实际数据桶的地址数值。值本身存储的地址就是基于哈希函数的计算结果，而搜索的过程就是利用哈希函数从元数据中推导出桶的地址。

- 添加新值的流程，首先会根据哈希函数计算出存储数据的地址，如果该地址已经被占用，则添加新桶并重新计算哈希函数。
- 更新值的流程则是先搜索到目标值的地址，然后对该内存地址应用所需的操作。

哈希索引会在进行相等性测试（等或者不等）时候具有非常高的性能，但是在进行比较查询、Order By 等更为复杂的场景下就无能为力。



### B-Tree

读作B树。

B-树是一种多路自平衡的搜索树 它类似普通的平衡二叉树，不同的一点是B-树**允许每个节点有更多的子节点**

那么m阶 B-Tree 是满足下列条件的数据结构：
* 所有键值分布在整颗树中
* 搜索有可能在非叶子结点结束，在关键字全集内做一次查找,性能逼近二分查找
* 每个节点最多拥有m个子树
* 根节点至少有2个子树
* 分支节点至少拥有m/2颗子树（除根节点和叶子节点外都是分支节点）
* 所有叶子节点都在同一层、每个节点最多可以有m-1个key，并且以升序排列

![](G:\picture\blog\databases\index_btree.jpg)

根据 B-Tree 的定义，可知检索一次最多需要访问 h（树的高度） 个节点。

数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次 I/O 就可以完全载入。每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个节点只需一次 I/O。而检索的时候，一次检索最多需要 h-1 次 I/O（根节点常驻内存），因此 h 非常小（通常不超过 3）。

而红黑树这种结构，h 明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的 I/O 渐进复杂度也为 O(h)，效率明显比 B-Tree 差很多。



B-Tree存在的问题：

（1）每个节点中有key，也有data，而每一个页的存储空间是有限的，如果data数据较大时就会导致每个节点（即一个页）能存储的key的数量很小

（2）当存储的数据量很大时，同样1会导致B-Tree的深度较大，增加查询时的磁盘I/O次数，进而影响查询效率




### B+tree

B+Tree 是 的变种，有着比 B-Tree 更高的查询性能，其相较于 B-Tree 有了如下的变化：

- 有 m 个子树的节点包含有 m 个元素（B-Tree 中是 m-1）。
- 根节点和分支节点中不保存数据，只用于索引，所有数据都保存在叶子节点中。
- 所有分支节点和根节点都同时存在于子节点中，在子节点元素中是最大或者最小的元素。
- 叶子节点会包含所有的关键字，以及指向数据记录的指针，并且叶子节点本身是根据关键字的大小从小到大顺序链接。

一般在数据库系统或文件系统中使用的 B+Tree 结构都在经典 B+Tree 的基础上进行了优化，增加了顺序访问指针：

![](G:\picture\blog\databases\b+tree.jpg)

如上图所示，在 B+Tree 的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的 B+Tree。做这个优化的目的是为了提高区间访问的性能，例如下图中如果要查询 key 为从 3 到 8 的所有数据记录，当找到 3 后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。

![](G:\picture\blog\databases\b+tree.jpg)



### LSM Tree

Log-Structured Merge-Tree
传统的关系数据库存储采用 B+ 树，数据被按照特定方式放置，能大幅度提升读性能，但写性能下降。
而 no-sql 一般 将整个磁盘就看做事一个日志，在日志中存放永久性数据及其索引，每次都添加到日志末尾。

将数据添加到文件，因为完全顺序，所以写操作性能优秀。但从日志文件读一些数据将比写操作消耗更多的时间，需要倒序扫描直到找到所需内容。相当于牺牲了部分读性能换来了写性能的提升。

LSM Tree 则采取读写分离的策略，会优先保证写操作的性能；其数据首先存储内存中，而后需要定期 Flush 到硬盘上。LSM-Tree 通过内存插入与磁盘的顺序写，来达到最优的写性能，因为这会大大降低磁盘的寻道次数，一次磁盘 IO 可以写入多个索引块。HBase, Cassandra, RockDB, LevelDB, SQLite 等都是基于 LSM Tree 来构建索引的数据库；LSM Tree 的树节点可以分为两种，保存在内存中的称之为 MemTable, 保存在磁盘上的称之为 SSTable。

LSM-tree 的主要思想是划分不同等级的树。以两级树为例，可以想象一份索引数据由两个树组成，一棵树存在于内存，一棵树存在于磁盘。内存中的树可以可以是 AVL Tree 等结构；因为数据大小是不同的，没必要牺牲 CPU 来达到最小的树高度。而存在于磁盘的树是一棵 B-Tree。

![](G:\picture\blog\databases\LSM_tree1.jpg)

数据首先会插入到内存中的树。当内存中的树中的数据超过一定阈值时，会进行合并操作。合并操作会从左至右遍历内存中的树的叶子节点与磁盘中的树的叶子节点进行合并，当**被合并的数据量达到磁盘的存储页的大小时，会将合并后的数据持久化到磁盘**，同时更新父亲节点对叶子节点的指针。

![](G:\picture\blog\databases\b+tree2.jpg)

之前存在于磁盘的叶子节点被合并后，旧的数据并不会被删除，这些数据会拷贝一份和内存中的数据一起顺序写到磁盘。这会操作一些空间的浪费，但是，LSM-Tree 提供了一些机制来回收这些空间。磁盘中的树的非叶子节点数据也被缓存在内存中。数据查找会首先查找内存中树，如果没有查到结果，会转而查找磁盘中的树。有一个很显然的问题是，如果数据量过于庞大，磁盘中的树相应地也会很大，导致的后果是合并的速度会变慢。一个解决方法是建立各个层次的树，低层次的树都比 上一层次的树数据集大。假设内存中的树为 c0, 磁盘中的树按照层次一次为 `c1, c2, c3, ... ck-1, ck`。合并的顺序是 `(c0, c1), (c1, c2)...(ck-1, ck)`。