Tags:[k8s]

### kube-apiserver

`kube-apiserver` 作为整个集群的入口，接受外部的信号和请求所应该具备的基本功能。

首先，它对外提供接口，可处理来自客户端（无论我们在用的 `kubeclt` 或者 `curl` 或者其他语言实现的客户端）的请求，并作出响应。

`apiserver` 有个 `--secure-port` 的参数，通过这个参数来配置它将要监听在哪个端口，默认情况下是 `6443`。

它还有另一个参数 `--insecure-port` ，这个参数可将 `kube-apiserver` 绑定到其指定的端口上，且通过该端口访问时无需认证。

在生产环境中，建议将其设置为 `0` 以禁用该功能。另外，这个参数也已经被标记为废弃，将在之后版本中移除。如果未禁用该功能，建议通过防火墙策略禁止从外部访问该端口。该端口会绑定在 `--insecure-bind-address` 参数所设置的地址上，默认为 `127.0.0.1`。



#### 认证

获取集群版本号为例：`kubectl version`

其实也是向 `kube-apiserver` 发送了一个请求进行查询的，我们可以通过传递 `-v` 参数来改变 log level 。

```
[root@192.168.18.196 ~]#kubectl version -v 8
I0422 14:03:21.468519  119289 loader.go:359] Config loaded from file /root/.kube/config
I0422 14:03:21.469723  119289 round_trippers.go:416] GET https://192.168.18.196:6443/version?timeout=32s
I0422 14:03:21.469753  119289 round_trippers.go:423] Request Headers:
I0422 14:03:21.469770  119289 round_trippers.go:426]     Accept: application/json, */*
I0422 14:03:21.469785  119289 round_trippers.go:426]     User-Agent: kubectl/v1.14.0 (linux/amd64) kubernetes/641856d
I0422 14:03:21.483462  119289 round_trippers.go:441] Response Status: 200 OK in 13 milliseconds
I0422 14:03:21.483496  119289 round_trippers.go:444] Response Headers:
I0422 14:03:21.483510  119289 round_trippers.go:447]     Content-Type: application/json
I0422 14:03:21.483525  119289 round_trippers.go:447]     Content-Length: 263
I0422 14:03:21.483557  119289 round_trippers.go:447]     Date: Mon, 22 Apr 2019 06:03:21 GMT
I0422 14:03:21.484689  119289 request.go:942] Response Body: {
  "major": "1",
  "minor": "14",
  "gitVersion": "v1.14.0",
  "gitCommit": "641856db18352033a0d96dbc99153fa3b27298e5",
  "gitTreeState": "clean",
  "buildDate": "2019-03-25T15:45:25Z",
  "goVersion": "go1.12.1",
  "compiler": "gc",
  "platform": "linux/amd64"
}
Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.0", GitCommit:"641856db18352033a0d96dbc99153fa3b27298e5", GitTreeState:"clean", BuildDate:"2019-03-25T15:53:57Z", GoVersion:"go1.12.1", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.0", GitCommit:"641856db18352033a0d96dbc99153fa3b27298e5", GitTreeState:"clean", BuildDate:"2019-03-25T15:45:25Z", GoVersion:"go1.12.1", Compiler:"gc", Platform:"linux/amd64"}
```

通过日志就可以很明显看到，首先会加载 `$HOME/.kube/config` 下的配置，获的集群地址，进而请求 `/version` 接口，最后格式化输出。

我们使用 `curl` 去请求同样的接口：

```
[root@192.168.18.196 ~]#curl -k https://192.168.18.196:6443/version
{
  "major": "1",
  "minor": "14",
  "gitVersion": "v1.14.0",
  "gitCommit": "641856db18352033a0d96dbc99153fa3b27298e5",
  "gitTreeState": "clean",
  "buildDate": "2019-03-25T15:45:25Z",
  "goVersion": "go1.12.1",
  "compiler": "gc",
  "platform": "linux/amd64"
｝
```

。。



```
[root@192.168.18.196 ~]#kubectl get ns -v 8
I0422 14:05:28.946651  120268 loader.go:359] Config loaded from file /root/.kube/config
I0422 14:05:28.958909  120268 round_trippers.go:416] GET https://192.168.18.196:6443/api/v1/namespaces?limit=500
I0422 14:05:28.958954  120268 round_trippers.go:423] Request Headers:
I0422 14:05:28.958974  120268 round_trippers.go:426]     Accept: application/json;as=Table;v=v1beta1;g=meta.k8s.io, application/json
I0422 14:05:28.958992  120268 round_trippers.go:426]     User-Agent: kubectl/v1.14.0 (linux/amd64) kubernetes/641856d
I0422 14:05:28.974324  120268 round_trippers.go:441] Response Status: 200 OK in 15 milliseconds
I0422 14:05:28.974362  120268 round_trippers.go:444] Response Headers:
I0422 14:05:28.974378  120268 round_trippers.go:447]     Content-Length: 3154
I0422 14:05:28.974393  120268 round_trippers.go:447]     Date: Mon, 22 Apr 2019 06:05:28 GMT
I0422 14:05:28.974408  120268 round_trippers.go:447]     Content-Type: application/json
I0422 14:05:28.974519  120268 request.go:942] Response Body: ....
I0422 14:05:28.975618  120268 get.go:570] no kind is registered for the type v1beta1.Table in scheme "k8s.io/kubernetes/pkg/api/legacyscheme/scheme.go:29"
NAME              STATUS   AGE
default           Active   9d
kube-node-lease   Active   9d
kube-public       Active   9d
kube-system       Active   9d
work              Active   9d
work-helm-test    Active   7d
[root@192.168.18.196 ~]#curl -k https://192.168.18.196:6443/api/v1/namespaces
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "namespaces is forbidden: User \"system:anonymous\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope",
  "reason": "Forbidden",
  "details": {
    "kind": "namespaces"
  },
  "code": 403
```

看到这里，应该就很明显了，当前忽略掉认证过程的 `curl` 被判定为 `system:anonymous` 用户，而此用户不具备列出 `namespace` 的权限。 

那我们是否有其他办法使用 `curl` 获取资源呢？ 当然有，使用 `kubectl proxy` 可以在本地和集群之间创建一个代理，就像这样：

```
}[root@192.168.18.196 ~]#kubectl proxy &
[1] 121294
[root@192.168.18.196 ~]#Starting to serve on 127.0.0.1:8001

[root@192.168.18.196 ~]#curl http://127.0.0.1:8001/api/v1/namespaces
{
  "kind": "NamespaceList",
  "apiVersion": "v1",
  "metadata": {
    "selfLink": "/api/v1/namespaces",
    "resourceVersion": "1059239"
  },
  "items": [..
```



在 `staging/src/k8s.io/client-go/tools/clientcmd/loader.go` 中，有一个名为 `LoadFromFile` 的函数用来提供加载配置文件的功能。



#### 授权

K8S 支持多种授权机制，现在多数都在使用 `RBAC` ，我们之前使用 `kubeadm` 创建集群时，默认会开启 `RBAC`



#### 准入控制

在请求进来时，会先经过认证、授权接下来会进入准入控制环节。准入控制和前两项内容不同，它不只是关注用户和行为，它还会处理请求的内容。不过它对读操作无效。

准入控制与我们前面说提到的认证、授权插件类似，支持同时开启多个。在 `v1.11.3` 中，默认开启的准入控制插件有：

```
NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeClaimResize,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,Priority
```

相关的代码可查看 `pkg/kubeapiserver/options/plugins.go`



`kube-apiserver` 包含的东西有很多，当你在终端下执行 `./kube-apiserver -h`



### etcd

`etcd` 是由 CoreOS 团队发起的一个分布式，强一致的键值存储。它用 Go 语言编写，使用 `Raft` 协议作为一致性算法。多数情况下会用于分布式系统中的服务注册发现，或是用于存储系统的关键数据。

`tcd` 在 K8S 中，最主要的作用便是其高可用，强一致的键值存储以及监听机制。

在 `kube-apiserver` 收到对应请求经过一系列的处理后，最终如果是集群所需要存储的数据，便会存储至 `etcd` 中。主部分主要是集群状态信息和元信息。

```
[root@192.168.18.196 ~]#kubectl -n kube-system get pods | grep etcd
etcd-192.168.18.196                      1/1     Running                  1          9d
```



在某些极端情况下，也许你需要通过直接操作 `etcd` 集群去变更数据，这里没有介绍所有的操作命令，感兴趣的可以自行通过下方的链接看官方文档进行学习。

但通常情况下，不建议直接操作 `etcd` ，除非你已经明确自己在做什么。

另外，由于 `etcd` 集群使用 `Raft` 一致性算法，通常情况下 `etcd` 集群需要部署奇数个节点，如 3，5，7 等。`etcd` 集群维护也相对容易，很容易可以做成高可用集群。（这也是保障 K8S 集群高可用的重要一环）



### controller-manager

一句话来讲 `kube-controller-manager` 是一个嵌入了 K8S 核心控制循环的守护进程。

看详细情况： `kubectl -n kube-system describe pods -l component=kube-controller-manager`

通过 `kube-apiserver` 提供的信息持续的监控集群状态，并尝试将集群调整至预期的状态。

由于访问 `kube-apiserver` 也需要通过认证，授权等过程，所以可以看到上面启动 `kube-controller-manager` 时提供了一系列的参数。

当我们删除某一个pod时，它会使其再次创建：

```
master $ kubectl -n kube-system logs -l component=kube-controller-manager --tail=5
I1210 09:30:17.125377       1 node_lifecycle_controller.go:945] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1210 09:31:07.140539       1 node_lifecycle_controller.go:972] Controller detected that some Nodes are Ready. Exiting master disruption mode.
I1210 09:43:30.377649       1 event.go:221] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"redis", UID:"0d1cb2d7-fc60-11e8-a361-0242ac110074", APIVersion:"apps/v1", ResourceVersion:"1494", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica setredis-bb7894d65 to 1
I1210 09:43:30.835149       1 event.go:221] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"redis-bb7894d65", UID:"0d344d15-fc60-11e8-a361-0242ac110074", APIVersion:"apps/v1", ResourceVersion:"1495", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod:redis-bb7894d65-w2rsp
I1210 09:47:41.658781       1 event.go:221] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"redis-bb7894d65", UID:"0d344d15-fc60-11e8-a361-0242ac110074", APIVersion:"apps/v1", ResourceVersion:"1558", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod:redis-bb7894d65-62ftk
```



在`10252` 端口上不仅暴露出来了一个 `/healthz` 接口，还暴露出了一个 `/metrics` 的接口，可用于进行监控之类的。



### kube-scheduler

`Scheduler` 则是集群调度器，将预期的 `Pod` 资源调度到正确的 `Node` 节点上，进而令该 `Pod` 可完成启动



### kubelet

我们知道了 K8S 中 Node 由一些必要的组件构成，而其中最为核心的当属 `kubelet` 了，如果没有 `kubelet` 的存在，那我们预期的各类资源就只能存在于 `Master`的相关组件中了，而 K8S 也很能只是一个 CRUD 的普通程序了。

按照一般架构设计上的习惯，`kubelet` 所承担的角色一般会被叫做 `agent`，`kubelet` 便是 K8S 中的 `agent`，负责 `Node` 和 `Pod` 相关的管理任务。



#### 节点管理

通常来讲 `agent` 这样的角色起到的作用首先便是要能够注册，让 `server` 端知道它的存在，所以这便是它的第一个作用：节点管理。

我们执行 `kubelet --help` 的时候，会看到它所支持的可配置参数，其中有一个 `--register-node`参数便是用于控制是否向 `kube-apiserver` 注册节点的，默认是开启的。

当 `kubeadm join` 执行成功后，你便可以通过 `kubectl get node` 查看到新加入集群中的 `Node`，与此同时，你也可以在该节点上通过以下命令查看 `kubelet`的状态。

```
master $ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Agent
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─kubeadm.conf
   Active: active (running) since Thu 2018-12-13 07:49:51 UTC; 32min ago
     Docs: http://kubernetes.io/docs/
 Main PID: 3876259 (kubelet)
   Memory: 66.3M
   CGroup: /system.slice/kubelet.service
           └─3876259 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernete...
```

`kubelet` 不仅将自己注册给了 `kube-apiserver`，同时它所在机器的信息也都进行了上报，包括 CPU，内存，IP 信息等。

`kubelet` 监听在了 `10250` 端口，这个端口可通过 `--port` 配置，但是之后会被废弃掉，我们是写入了 `/var/lib/kubelet/config.yaml` 的配置文件中。



#### pod 管理

从上面的配置以及我们之前的介绍中，`kube-scheduler` 处理了 `Pod` 应该调度至哪个 `Node`，而 `kubelet` 则是保障该 `Pod` 能按照预期，在对应 `Node` 上启动并保持工作。

同时，`kubelet` 在保障 `Pod` 能按预期工作，主要是做了两方面的事情：

- 健康检查：通过 `LivenessProbe` 和 `ReadinessProbe` 探针进行检查，判断是否健康及是否已经准备好接受请求。
- 资源监控：通过 `*cAdvisor*` 进行资源监。



### kube-proxy

`kube-proxy` 是 K8S 运行于每个 `Node` 上的网络代理组件，提供了 TCP 和 UDP 的连接转发支持。

我们已经知道，当 `Pod` 在创建和销毁的过程中，IP 可能会发生变化，而这就容易造成对其有依赖的*服务*的异常，所以通常情况下，我们都会使用 `Service` 将后端 `Pod` 暴露出来，而 `Service` 则较为稳定。

#### 增加一个监听端口

查看当前集群的 `Service` 和 `Endpoint`

```
master $ kubectl -n work get svc
NAME           TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
saythx-redis   NodePort   10.103.193.175   <none>        6379:31269/TCP   10m
master $ kubectl -n work get endpoints
NAME           ENDPOINTS        AGE
saythx-redis   10.32.0.2:6379   10m
master $ kubectl -n work get pod -o wide
NAME                           READY     STATUS    RESTARTS   AGE       IP          NODE      NOMINATED NODE
saythx-redis-8558c7d7d-wsn2w   1/1       Running   0          12m       10.32.0.2   node01    <none>
```

可以很直观的看到 `Endpoint` 当中的便是 `Pod` 的 IP，现在我们将该服务进行扩容（实际情况下并不会这样处理）。

直接通过 `kubectl scale` 操作

```
master $ kubectl  -n work scale --replicas=2 deploy/saythx-redis
deployment.extensions/saythx-redis scaled
master $ kubectl  -n work get all
NAME                               READY     STATUS    RESTARTS   AGE
pod/saythx-redis-8558c7d7d-sslpj   1/1       Running   0          10s
pod/saythx-redis-8558c7d7d-wsn2w   1/1       Running   0          16m

NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/saythx-redis   NodePort   10.103.193.175   <none>        6379:31269/TCP   16m

NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/saythx-redis   2         2         2            2           16m
```

查看 `Endpoint` 信息：

```
master $ kubectl -n work get endpoints
NAME           ENDPOINTS                       AGE
saythx-redis   10.32.0.2:6379,10.32.0.3:6379   17m
```

可以看到 `Endpoint` 已经自动发生了变化，而这也意味着 `Service` 代理的后端节点将增加一个。



#### 如何工作

`kube-proxy` 在 Linux 系统上当前支持三种模式，可通过 `--proxy-mode` 配置：

- `userspace`：这是很早期的一种方案，但效率上显著不足，不推荐使用。
- `iptables`：当前的默认模式。比 `userspace` 要快，但问题是会给机器上产生很多 `iptables` 规则。
- `ipvs`：为了解决 `iptables` 的性能问题而引入，采用增量的方式进行更新。



### Container Runtime （Docker）

`kube-scheduler` 决定了 `Pod` 将被调度到哪个 `Node` 上，而 `kubelet` 则负责 `Pod` 在此 `Node` 上可按预期工作。如果没有 `Container Runtime`，那 `Pod` 中的 `container` 在该 `Node` 上也便无法正常启动运行了。

我们以当前最为通用的 `Container Runtime` Docker 为例进行介绍，

`Container Runtime` 我们通常叫它容器运行时，而这一概念的产生也是由于容器化技术和 K8S 的大力发展，为了统一工业标准，也为了避免 K8S 绑定于特定的容器运行时，所以便成立了 Open Container Initiative (OCI)组织，致力于将容器运行时标准化和容器镜像标准化



#### docker

Docker 是一个容器管理平台，它最初是被设计用于快速创建，发布和运行容器的工具，不过随着它的发展，其中集成了越来越多的功能。

Docker 也可以说是一个包含标准容器运行时的工具集，当前版本中默认的 `runtime` 称之为 `runc`。

在 `Docker` 中，当你使用 `docker info` 即可查看当前所使用的 runtime。

当然，这里提到了 **默认的运行时** 那也就意味着它可支持其他的运行时实现。



#### CRI

也不会将自己完全局限于某一种特定的容器运行时。

自 K8S 1.5 （2016 年 11 月）开始，新增了一个容器运行时的插件 API，并称之为 `CRI` （Container Runtime Interface），通过 `CRI` 可以支持 `kubelet` 使用不同的容器运行时，而不需要重新编译。

`CRI` 主要是基于 gRPC 实现了 `RuntimeService` 和 `ImageService` 这两个服务，可以参考 `pkg/kubelet/apis/cri/runtime/v1alpha2/api.proto` 中的 API 定义。由于本节侧重于 `Container Runtime/Docker` 这里就不对 `CRI` 的具体实现进行展开了。

只要继续将 `kubelet` 当作 agent 的角色，而它与基于 `CRI` 实现的 `CRI shim` 服务进行通信理解即可。



