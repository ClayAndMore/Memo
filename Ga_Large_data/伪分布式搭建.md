

### 构建目录

构建目录如下：

```shell
[root@192.168.18.198 /]#tree /opt/
/opt/
├── moudles
│   ├── apache
│   └── cdh
└── software   # 存放安装包
```

java安装和配置：

```shell
tar xvfz jdk-7u67-linux-x64.tar.gz /opt/moudles/
#pwd
/opt/moudles/jdk1.7.0_67
#vim /etc/profile
export JAVA_HOME=/opt/moudles/jdk1.7.0_67
export PATH=$PATH:$JAVA_HOME/bin

#source /etc/profile
#jps
70407 Jps
```



安装hadoop:

```shell
tar xvfz hadoop-2.7.3.tar.gz -C /opt/moudles/apache/
```



### 配置

目录：/opt/moudles/apache/hadoop-2.7.3/etc/hadoop

#### 配置JAVA_HOME

hadoop-env.sh:

```
export JAVA_HOME=${JAVA_HOME}
export JAVA_HOME=/opt/moudles/jdk1.7.0_67
```

mapred-env.sh:

```
# export JAVA_HOME=/home/y/libexec/jdk1.6.0/
export JAVA_HOME=/opt/moudles/jdk1.7.0_67
```

yarn-env.sh 

```
# export JAVA_HOME=/home/y/libexec/jdk1.6.0/
export JAVA_HOME=/opt/moudles/jdk1.7.0_67
```



#### host

如果改变host, 记得更新/etc/hosts:

```
hostnamectl set-hostname node198
[root@node198 hadoop-2.7.3]#cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

127.0.0.1 node198
```





#### 配置hdfs相关

core-site.xml：

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node198:9000</value>
    </property>
     <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/moudles/apache/hadoop-2.7.3/data/tmp</value>
    </property>
</configuration>
```

配置了主节点namenode的访问入口，是hostname+ 端口号 

元数据 的配置目录， data目录不需要手动创建。

更多和默认值可看：<https://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-common/core-default.xml>

eg:  `hadoop.tmp.dir: /tmp/hadoop-${user.name}`

hdfs-site.xml

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

配置副本， 更多和默认值可看：<https://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml

或本地机器可看： ./share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml

slaves: 

`node198`



### 启动

格式化命令：`bin/hdfs namenode -format`

19/06/10 15:37:00 INFO common.Storage: Storage directory /opt/moudles/apache/hadoop-2.7.3/data/tmp/dfs/name has been successfully formatted.

启动：

```shell
[root@node198 hadoop-2.7.3]#sbin/hadoop-daemon.sh start namenode
starting namenode, logging to /opt/moudles/apache/hadoop-2.7.3/logs/hadoop-root-namenode-node198.out
[root@node198 hadoop-2.7.3]#sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /opt/moudles/apache/hadoop-2.7.3/logs/hadoop-root-datanode-node198.out
[root@node198 hadoop-2.7.3]#
[root@node198 hadoop-2.7.3]#jps
21687 DataNode
21807 Jps
21504 NameNode
```



关闭： `sbin/hadoop-daemon.sh stop namenode`

killall java, 没有这个命令，安装：`yum install psmisc`



### web 页面

<https://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml>， 搜http，可以看到开放了一些端口。

eg: http://ip:50070

记得关防火墙：`systemctl stop firewalld`



### 配置SecondaryNode

```
[root@node198 hadoop-2.7.3]#killall java
[root@node198 hadoop-2.7.3]#jps
37521 Jps
```

hdfs-site.xml加入：

```xml
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node198:50090</value>
    </property>
```

再次启动：

```
[root@node198 hadoop-2.7.3]#sbin/hadoop-daemon.sh start namenode
starting namenode, logging to /opt/moudles/apache/hadoop-2.7.3/logs/hadoop-root-namenode-node198.out
[root@node198 hadoop-2.7.3]#sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /opt/moudles/apache/hadoop-2.7.3/logs/hadoop-root-datanode-node198.out
[root@node198 hadoop-2.7.3]#sbin/hadoop-daemon.sh start secondarynamenode
starting secondarynamenode, logging to /opt/moudles/apache/hadoop-2.7.3/logs/hadoop-root-secondarynamenode-node198.out
[root@node198 hadoop-2.7.3]#jps
38403 Jps
38171 DataNode
38035 NameNode
38337 SecondaryNameNode
```



工作机制：

![](http://211.159.177.235:5000/uploads/big/4b0844fce9d78bbf86634b9896f2684a.png)



namenode 的元数据信息先往edits文件写， 当edits文件到达一定阈值（3600s）时，会开启合并的流程：

1. secondarynamenode 会吧edits和fsimage拷贝到自己服务器所在的内存中，开始合并，合并生成一个新的fsiamge.ckpt.
2. 将fsimage.ckpt 文件拷贝到namenode上，成功后，再删除原有的文件， 将fsimage.ckpt重命名为fsiamge
3. sencondarynamenode 将 edist 和 fsiamge拷贝走后，namenode 会立刻生成edits.new, 用于记录新来的元数据信息， 当合并完成之后，原有的edits文件才会删除，并将edits.new重命名为edits,开启下一轮的合并

优化配置

1. 配置单独的fsimage路径：

   ```xml
   <property>
     <name>dfs.namenode.name.dir</name>
     <value>file://${hadoop.tmp.dir}/dfs/name</value>
   </property>
   ```

2. 配置单独的edits文件存放路径

   ```xml
   <property>
     <name>dfs.namenode.edits.dir</name>
     <value>${dfs.namenode.name.dir}</value>
   </property>
   ```

3. 指定datanode数据本地存放路径

   ```xml
   <property>
     <name>dfs.datanode.data.dir</name>
     <value>file://${hadoop.tmp.dir}/dfs/data</value>
   </property>
   ```

   

