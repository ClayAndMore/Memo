---
title: "07_ha高可用性能搭建.md"
date: 2019-09-29 17:53:13 +0800
lastmod: 2019-09-29 17:53:13 +0800
draft: false
tags: ["hadoop"]
categories: ["大数据"]
author: "Claymore"

---
Tags:[大数据] 

#### 环境变量

/etc/profile

```
export JAVA_HOME=/opt/module/jdk1.8.0_211
export PATH=$PATH:$JAVA_HOME/bin

export HADOOP_HOME=/opt/module/hadoop-3.1.2
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```





hadoop版本3.2

官方文档：https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html



HA

| -      | NN-1 | NN-2 | DN   | ZK   | ZKFC | JNN  |
| ------ | ---- | ---- | ---- | ---- | ---- | ---- |
| Node01 | *    |      |      |      | *    | *    |
| Node02 |      | *    | *    | *    | *    | *    |
| Node03 |      |      | *    | *    |      | *    |
| Node04 |      |      | *    | *    |      |      |

这里以node191对应node01



#### 脑裂 split-brain

```

           +----------------------------------+
           |                                  |
        +->+         zk                        +--------+
     kfc|  |                                  |       |kfc
        |  +----------------------------------+       |
        |                                             |
        |                                             |
+---------+                                   +-------v----+
| active+ |                                   |  standby   |
|         |                                   |            |
+---------+                                   +------------+
   nn1                                               nn2
```

当nn1 的kfc和zk连接终端，此时nn1仍是active 状态， 但是zk接收到了nn1的异常，如果这时zk通知nn2的kfc，将其由状态standby变为active,则此时有两个nn, 这种情况称之为脑裂。







### 配置文件



#### hadoop-env.sh:

```
export JAVA_HOME=
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root

export HDFS_ZKFC_USER=root
export HDFS_JOURNALNODE_USER=root
```



#### hdfs-site.xml

```xml
<property>
    <name>dfs.replication</name>
    <value>3</value>
</property>
<property>
    <name>dfs.nameservices</name>
    <value>mycluster</value>
</property>
<property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>nn1,nn2</value>
</property>
<property>
    <name>dfs.namenode.rpc-address.mycluster.nn1</name>
    <value>node191:8020</value>
</property>
<property>
    <name>dfs.namenode.rpc-address.mycluster.nn2</name>
    <value>node192:8020</value>
</property>
<property>
  <name>dfs.namenode.http-address.mycluster.nn1</name>
  <value>node191:9870</value>
</property>
<property>
  <name>dfs.namenode.http-address.mycluster.nn2</name>
  <value>node192:9870</value>
</property>

<property>
  <name>dfs.namenode.shared.edits.dir</name>
  <value>qjournal://node191:8485;node192:8485;node193:8485/mycluster</value>
</property>

<property>
  <name>dfs.client.failover.proxy.provider.mycluster</name>
  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>

<property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
</property>

<property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/root/.ssh/id_rsa</value>
</property>

<property>
  <name>dfs.journalnode.edits.dir</name>
  <value>/opt/module/hadoop-3.1.2/data/journalnode</value>
</property>

 <property>
   <name>dfs.ha.automatic-failover.enabled</name>
   <value>true</value>
 </property>
```

**dfs.nameservices** - the logical name for this new nameservice

Choose a logical name for this nameservice, for example “mycluster”, and use this logical name for the value of this config option. The name you choose is arbitrary. It will be used both for configuration and as the authority component of absolute HDFS paths in the cluster.

**Note:** If you are also using HDFS Federation(联邦), this configuration setting should also include the list of other nameservices, HA or otherwise, as a comma-separated list.

namenode变多 我们需要一个逻辑名称指定, 例子中指定的是mycluster这个名字。

**dfs.ha.namenodes.[nameservice ID]** - unique identifiers for each NameNode in the nameservice

Configure with a list of comma-separated NameNode IDs. This will be used by DataNodes to determine all the NameNodes in the cluster. For example, if you used “mycluster” as the nameservice ID previously, and you wanted to use “nn1”, “nn2” and “nn3” as the individual IDs of the NameNode

再次指定namenode的逻辑名字所包含的nn。

**dfs.namenode.rpc-address.[nameservice ID].[name node ID]** - the fully-qualified RPC address for each NameNode to listen on

For both of the previously-configured NameNode IDs, set the full address and IPC port of the NameNode processs

rpc配置， 指定上方逻辑名称的物理名称。

**dfs.namenode.http-address.[nameservice ID].[name node ID]** - the fully-qualified HTTP address for each NameNode to listen on

Similarly to *rpc-address* above, set the addresses for both NameNodes’ HTTP servers to listen on.



**dfs.namenode.shared.edits.dir** - the URI which identifies the group of JNs where the NameNodes will write/read edits

This is where one configures the addresses of the JournalNodes which provide the shared edits storage, written to by the Active nameNode and read by the Standby NameNode to stay up-to-date with all the file system changes the Active NameNode makes. Though you must specify several JournalNode addresses, **you should only configure one of these URIs.** The URI should be of the form: `qjournal://*host1:port1*;*host2:port2*;*host3:port3*/*journalId*`. The Journal ID is a unique identifier for this nameservice, which allows a single set of JournalNodes to provide storage for multiple federated namesystems. Though not a requirement, it’s a good idea to reuse the nameservice ID for the journal identifier.

For example, if the JournalNodes for this cluster were running on the machines “node1.example.com”, “node2.example.com”, and “node3.example.com” and the nameservice ID were “mycluster”, you would use the following as the value for this setting (the default port for the JournalNode is 8485)



**dfs.client.failover.proxy.provider.[nameservice ID]** - the Java class that HDFS clients use to contact the Active NameNode

Configure the name of the Java class which will be used by the DFS Client to determine which NameNode is the current Active, and therefore which NameNode is currently serving client requests. The two implementations which currently ship with Hadoop are the **ConfiguredFailoverProxyProvider** and the **RequestHedgingProxyProvider** (which, for the first call, concurrently invokes all namenodes to determine the active one, and on subsequent requests, invokes the active namenode until a fail-over happens), so use one of these unless you are using a custom proxy provider.

故障转移代理类



**dfs.ha.fencing.methods** - a list of scripts or Java classes which will be used to fence the Active NameNode during a failover

It is desirable for correctness of the system that only one NameNode be in the Active state at any given time. **Importantly, when using the Quorum Journal Manager, only one NameNode will ever be allowed to write to the JournalNodes, so there is no potential for corrupting the file system metadata from a split-brain scenario.** However, when a failover occurs, it is still possible that the previous Active NameNode could serve read requests to clients, which may be out of date until that NameNode shuts down when trying to write to the JournalNodes. For this reason, it is still desirable to configure some fencing methods even when using the Quorum Journal Manager. However, to improve the availability of the system in the event the fencing mechanisms fail, it is advisable to configure a fencing method which is guaranteed to return success as the last fencing method in the list. Note that if you choose to use no actual fencing methods, you still must configure something for this setting, for example “`shell(/bin/true)`”.

The fencing methods used during a failover are configured as a carriage-return-separated list, which will be attempted in order until one indicates that fencing has succeeded. There are two methods which ship with Hadoop: *shell* and *sshfence*. For information on implementing your own custom fencing method, see the *org.apache.hadoop.ha.NodeFencer* class.

配置私钥来登录两个nn，防止脑裂



The configuration of automatic failover requires the addition of two new parameters to your configuration, 自导



#### core-site.xml

```xml
<property>
        <name>fs.defaultFS</name>
        <value>hdfs://mycluster</value>
</property>
<property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/moudles/hadoop-3.1.2/data/tmp</value>
</property>
<property>
        <name>hadoop.http.staticuser.user</name>
        <value>root</value>
</property>
 <property>
   <name>ha.zookeeper.quorum</name>
   <value>node192:2181,node193:2181,node194:2181</value>
 </property>
```

配置zk集群。

#### works
```
node192
node193
node194
```



分发更改的配置文件：

```
scp hadoop-env.sh core-sit.xml hdfs-site.xml  works root@node192: `pwd`
scp hadoop-env.sh core-sit.xml hdfs-site.xml  works root@node193: `pwd`
scp hadoop-env.sh core-sit.xml hdfs-site.xml  works root@node194: `pwd`
```






### 安装zk

官网下载包，放到/opt/module, 解压： `tar xvfz zookeeper-3.4.6.tar.gz`

同级创建状态数据存储目录： mkidir zkdata

配置zoo.cfg文件：

```shell
cd /opt/module/zookeeper-3.4.6/conf
cp zoo_sample.cfg zoo.cfg
vim zoo.cfg:

dataDir=/opt/module/zkdata
clientPort=2181

# 各个服务节点地址配置
server.1=node191:2888:3888
server.2=node192:2888:3888
server.3=node193:2888:3888
```

创建myid文件，在dataDir下，写入0-225的整数，相应节点对应的是上方配置中的数字

配置环境变量：/etc/profile

```shell
export ZK_HOME=/opt/module/zookeeper-3.4.6
export PATH=$PATH:$ZK_HOME/bin
```

Source /etc/profile

Node193,node194机器也这么干。

启动：

```shell
root@node192 zookeeper-3.4.6]# zkServer.sh start
JMX enabled by default
Using config: /opt/module/zookeeper-3.4.6/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@node192 zookeeper-3.4.6]# zkServer.sh status
JMX enabled by default
Using config: /opt/module/zookeeper-3.4.6/bin/../conf/zoo.cfg
Mode: follower

[root@node193 ~]# zkServer.sh status
JMX enabled by default
Using config: /opt/module/zookeeper-3.4.6/bin/../conf/zoo.cfg
Mode: leader
```

三个节点都这样干，这里为啥193是主节点？,一般来说序号的数字越大称为leader的机会越高，可能是我手工start，太慢，起到node193的时候就已经选出leader了。

停止 ： zkServer.sh stop



### zk客户端

zkCli.sh

输入help，可以看些命令。

eg:

```
[zk: localhost:2181(CONNECTED) 0] ls /
[zookeeper]
[zk: localhost:2181(CONNECTED) 1]
```





### 启动

#### journalnode

先启动journalnode: hdfs --daemon start journalnode,  在 node191, node192, node193

启动后 jps看下。

```
# node191
jps
7829 Jps
7768 JournalNode

# node192
7574 QuorumPeerMain
8074 Jps
8045 JournalNode
```

因为 192有zk。

此时 data文件夹下会有个新journalnode 文件夹。



#### namenode 格式化

 格式化nn1(node191)：  `hdfs namenode -format` 

格式化后看下data文件夹下有没有多个tmp文件夹

启动namenode:

```shell
[root@node191 hadoop]# hadoop-daemon.sh start namenode
WARNING: Use of this script to start HDFS daemons is deprecated.
WARNING: Attempting to execute replacement "hdfs --daemon start" instead.
[root@node191 hadoop]# jps
8323 NameNode
8362 Jps
8012 JournalNode
```

后， 格式化nn2 : `hdfs namenode -bootstrapStandby`

成功后输出：

```
2019-06-12 08:46:34,071 INFO ha.BootstrapStandby: Found nn: nn1, ipc: node191/192.168.145.191:8020
=====================================================
About to bootstrap Standby ID nn2 from:
           Nameservice ID: mycluster
        Other Namenode ID: nn1
  Other NN's HTTP address: http://node191:9870
  Other NN's IPC  address: node191/192.168.145.191:8020
             Namespace ID: 2017075246
            Block pool ID: BP-934550845-192.168.145.191-1560309554232
               Cluster ID: CID-efe6ac54-4ceb-484a-ac87-2c815facba79
           Layout version: -64
       isUpgradeFinalized: true
=====================================================
2019-06-12 08:46:36,405 INFO common.Storage: Storage directory /opt/module/hadoop-3.1.2/data/tmp/dfs/name has been successfully formatted.
2019-06-12 08:46:36,576 INFO namenode.FSEditLog: Edit logging is async:true
2019-06-12 08:46:36,804 INFO namenode.TransferFsImage: Opening connection to http://node191:9870/imagetransfer?getimage=1&txid=0&storageInfo=-64:2017075246:1560309554232:CID-efe6ac54-4ceb-484a-ac87-2c815facba79&bootstrapstandby=true
2019-06-12 08:46:36,998 INFO common.Util: Combined time for file download and fsync to all disks took 0.01s. The file download took 0.01s at 0.00 KB/s. Synchronous (fsync) write to disk of /opt/module/hadoop-3.1.2/data/tmp/dfs/name/current/fsimage.ckpt_0000000000000000000 took 0.00s.
2019-06-12 08:46:36,998 INFO namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 391 bytes.
2019-06-12 08:46:37,096 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at node192/192.168.145.192
************************************************************/
```

成功后可以看下这个第二个节点的custerID是否和node191一样：

```
[root@node192 ~]# cat /opt/module/hadoop-3.1.2/data/tmp/dfs/name/current/VERSION
#Wed Jun 12 08:46:36 EDT 2019
namespaceID=2017075246
clusterID=CID-efe6ac54-4ceb-484a-ac87-2c815facba79
cTime=1560309554232
storageType=NAME_NODE
blockpoolID=BP-934550845-192.168.145.191-1560309554232
layoutVersion=-64
```



#### zk格式化

Node191: `hdfs zkfc -formatZK`

Node192, 会发现多了个hadoop-ha:

```
[zk: localhost:2181(CONNECTED) 0] ls /
[zookeeper, hadoop-ha]
[zk: localhost:2181(CONNECTED) 1] ls /hadoop-ha
[mycluster]
[zk: localhost:2181(CONNECTED) 2] ls /hadoop-ha/mycluster
[]
```



Start-dfs.sh:

```
[root@node191 ~]# start-dfs.sh
Starting namenodes on [node191 node192]
Last login: Wed Jun 12 09:44:38 EDT 2019 from 192.168.145.1 on pts/0
node191: Warning: Permanently added 'node191,192.168.145.191' (ECDSA) to the list of known hosts.
node191: namenode is running as process 9522.  Stop it first.
Starting datanodes
Last login: Wed Jun 12 09:44:50 EDT 2019 on pts/0
Starting journal nodes [node191 node193 node192]
Last login: Wed Jun 12 09:44:53 EDT 2019 on pts/0
node191: journalnode is running as process 8012.  Stop it first.
node193: journalnode is running as process 8193.  Stop it first.
node192: journalnode is running as process 8194.  Stop it first.
Starting ZK Failover Controllers on NN hosts [node191 node192]
Last login: Wed Jun 12 09:45:06 EDT 2019 on pts/0
[root@node191 ~]# jps
10666 DFSZKFailoverController
10714 Jps
8012 JournalNode
```

会发现node192:

```
[zk: localhost:2181(CONNECTED) 3] ls /hadoop-ha/mycluster
[ActiveBreadCrumb, ActiveStandbyElectorLock]

[zk: localhost:2181(CONNECTED) 4] get /hadoop-ha/mycluster/ActiveBreadCrumb # 获取注册信息

	myclusternn1node191 �>(�>
cZxid = 0x400000008
ctime = Wed Jun 12 09:45:21 EDT 2019
mZxid = 0x400000008
mtime = Wed Jun 12 09:45:21 EDT 2019
pZxid = 0x400000008
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 31
numChildren = 0
```



几个操作观察主从nn的切换：

node191: `hdfs —daemon stop namenode`

观察node191,node192（端口9870）页面, 看状态是active,或standby,或已经进不去

node191: `hdfs —daemon start namenode`

Node192: `hdfs -daemon stop zkfc`

再观察页面