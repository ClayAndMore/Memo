---

title: "网络.md"
date: 2020-05-15 18:43:09 +0800
lastmod: 2020-06-12 19:01:02 +0800
draft: false
tags: [""]
categories: ["k8s"]
author: "Claymore"

---


## Service

**Service` 简单点说就是为了能有个稳定的入口访问我们的应用服务或者是一组 `Pod**。通过 `Service`可以很方便的实现服务发现和负载均衡

```sh
[root@localhost k8s]# kubectl get service -o wide
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   145m   <none>
```

通过使用 `kubectl` 查看，能看到主要会显示 `Service` 的名称，类型，IP，端口及创建时间和选择器等。我们来具体拆解下。

### 类型

`Service` 目前有 4 种类型：

- `ClusterIP`： 是 K8S 当前默认的 `Service` 类型。将 service 暴露于一个仅集群内可访问的虚拟 IP 上。
- `NodePort`： 是通过在集群内所有 `Node` 上都绑定固定端口的方式将服务暴露出来，这样便可以通过 `<NodeIP>:<NodePort>` 访问服务了。
- `LoadBalancer`： 是通过 `Cloud Provider` 创建一个外部的负载均衡器，将服务暴露出来，并且会自动创建外部负载均衡器路由请求所需的 `Nodeport` 或 `ClusterIP` 。
- `ExternalName`： 是通过将服务由 DNS CNAME 的方式转发到指定的域名上将服务暴露出来，这需要 `kube-dns` 1.7 或更高版本支持。

**service 可以简写成 svc**



## kube-proxy

**`kube-proxy` 是 K8S 运行于每个 `Node` 上的网络代理组件，提供了 TCP 和 UDP 的连接转发支持。**

我们已经知道，当 `Pod` 在创建和销毁的过程中，IP 可能会发生变化，而这就容易造成对其有依赖的服务的异常，所以通常情况下，我们都会使用 `Service` 将后端 `Pod` 暴露出来，而 `Service` 则较为稳定。

正常我们的service 是这样的：

``` sh
root@node200:~# kubectl get svc -o wide -A
NAMESPACE     NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE     SELECTOR
default       kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  7m19s   <none>
kube-system   kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   7m17s   k8s-app=kube-dns
```

创建一个nginx 端口转发的service:

nginx-rc.yaml:

``` yaml
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: default
spec:
  selector:
    matchLabels:
      name: nginx
  replicas: 1 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
```

nginx-service.yaml:

``` yaml
apiVersion: v1
kind: Service
metadata:
  namespace: default
  name: nginx-deployment
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
      nodePort: 30080
  selector:
    name: nginx
```

使用 kubectl apply -f 将上面两个yaml配置到k8s中，在此查看服务状态：

``` sh
root@node200:~x# kubectl get svc -o wide -A
NAMESPACE     NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE   SELECTOR
default       kubernetes         ClusterIP   10.96.0.1        <none>        443/TCP                  48m   <none>
default       nginx-deployment   NodePort    10.105.209.221   <none>        80:30080/TCP             82s   name=nginx
kube-system   kube-dns           ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP   48m   k8s-app=kube-dns

root@node200:~# netstat  -ntlp |grep 30080
tcp6       0      0 :::30080                :::*                    LISTEN      7293/kube-proxy
```

可以看到该端口是由 `kube-proxy` 所占用的。

看下此时nginx pod：

``` sh
root@node200:~/workspace/trireme-k8s/nginx# kubectl get pods -A -o wide
NAMESPACE     NAME                                READY   STATUS    RESTARTS   AGE   IP              NODE      NOMINATED NODE   READINESS GATES
default       nginx-deployment-85dfbdd967-482hs   1/1     Running   0          10s   10.244.0.3      node201   <none>           <none>


kube-system   kube-proxy-7h9sh                    1/1     Running   0          55m   172.19.19.201   node201   <none>           <none>
kube-system   kube-proxy-jgf7g                    1/1     Running   0          60m   172.19.19.200   node200   <none>           <none>
....
```

**注意这里 kube-porxy 在每个节点有一个**， 10.244 IP 是使用flannel指定的 --pod-network-cidr=10.244.0.0/16。



### kube-proxy 如何工作

`kube-proxy` 在 Linux 系统上当前支持三种模式，可通过 `--proxy-mode` 配置：

- `userspace`：这是很早期的一种方案，但效率上显著不足，不推荐使用。
- `iptables`：当前的默认模式。比 `userspace` 要快，但问题是会给机器上产生很多 `iptables` 规则。
- `ipvs`：为了解决 `iptables` 的性能问题而引入，采用增量的方式进行更新。

``` sh
# iptables -t nat -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
KUBE-SERVICES  all  --  anywhere             anywhere             /* kubernetes service portals */
DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL

Chain KUBE-SERVICES (2 references)
# 这里只摘了两条, 
target     prot opt source               destination
KUBE-MARK-MASQ  tcp  -- !10.244.0.0/16        10.105.209.221       /* default/nginx-deployment:http cluster IP */ tcp dpt:http
KUBE-SVC-SA755BF2EJWCXKH5  tcp  --  anywhere             10.105.209.221       /* default/nginx-deployment:http cluster IP */ tcp dpt:http

Chain KUBE-MARK-MASQ (14 references)
target     prot opt source               destination
MARK       all  --  anywhere             anywhere             MARK or 0x4000

Chain KUBE-SVC-SA755BF2EJWCXKH5 (2 references)
target     prot opt source               destination
KUBE-SEP-SDM4OGM2YG5A4OCW  all  --  anywhere             anywhere

Chain KUBE-SEP-SDM4OGM2YG5A4OCW (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.244.0.3           anywhere
DNAT       tcp  --  anywhere             anywhere             tcp to:10.244.0.3:80
```

在浏览器上访问nginx， 根据链的走向，先后经过 PREROUTING->KUBE-SERVICES-> KUBE-SVC-SA755BF2EJWCXKH5 ->KUBE-SEP-SDM4OGM2YG5A4OCW. 

最后通过 DNAT的方式走到 10.244.0.3:80

10.105.209.211 应该是为nginx pod 生成的随机ip:

``` sh
root@node200:~/workspace/trireme-k8s/nginx# kubectl describe svc/nginx-deployment -n default
Name:                     nginx-deployment
Namespace:                default
Labels:                   <none>
Annotations:              kubectl.kubernetes.io/last-applied-configuration:
                            {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"},"spec":{"ports":[{"name"...
Selector:                 name=nginx
Type:                     NodePort
IP:                       10.105.209.221
Port:                     http  80/TCP
TargetPort:               80/TCP
NodePort:                 http  30080/TCP
Endpoints:                10.244.0.3:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
root@node200:~/workspace/trireme-k8s/nginx# kubectl get endpoints -n default
NAME               ENDPOINTS            AGE
kubernetes         172.19.19.200:6443   66m
nginx-deployment   10.244.0.3:80        5m51s
```





## CoreDns/kube-dns

https://coredns.io/



 CoreDNS 是一个独立项目，它不仅可支持在 K8S 中使用，你也可以在你任何需要 DNS 服务的时候使用它。

CoreDNS 使用 Go 语言实现，部署非常方便。

它的扩展性很强，很多功能特性都是通过插件完成的，它不仅有大量的[内置插件](https://coredns.io/plugins/)，同时也有很丰富的[第三方插件](https://coredns.io/explugins/)。甚至你自己[写一个插件](https://coredns.io/2016/12/19/writing-plugins-for-coredns/)也非常的容易。

CoreDNS 在 K8S 1.13 版本中才正式成为[默认的 DNS 服务](https://kubernetes.io/blog/2018/12/03/kubernetes-1-13-release-announcement/)。



使用 CoreDNS 代替 kube-dns 主要是为了解决一些 kube-dns 时期的问题，比如说原先 kube-dns 的时候，一个 Pod 中还需要包含 `kube-dns`, `sidecar` 和 `dnsmasq` 的容器，而每当 `dnsmasq` 出现漏洞时，就不得不让 K8S 发个安全补丁才能进行更新。

CoreDNS 有丰富的插件，可以满足更多样的应用需求，同时 `kubernetes` 插件还包含了一些独特的功能，比如 Pod 验证之类的，可增加安全性。



### 查看

在观察pod svc 的时候我们可以发现 kube-dns 的踪迹：

``` sh
kube-system   kube-dns           ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP   48m   k8s-app=kube-dns
```

通过标签的方式查看 CoreDns:

``` sh
root@node200:~# kubectl -n kube-system get all  -l k8s-app=kube-dns -o wide
NAME                           READY   STATUS    RESTARTS   AGE    IP           NODE      NOMINATED NODE   READINESS GATES
pod/coredns-6955765f44-mvdfd   1/1     Running   0          168m   10.244.0.3   node200   <none>           <none>
pod/coredns-6955765f44-nt9gn   1/1     Running   0          168m   10.244.0.2   node200   <none>           <none>

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE    SELECTOR
service/kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   168m   k8s-app=kube-dns

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES                     SELECTOR
deployment.apps/coredns   2/2     2            2           168m   coredns      k8s.gcr.io/coredns:1.6.5   k8s-app=kube-dns

NAME                                 DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES                     SELECTOR
replicaset.apps/coredns-6955765f44   2         2         2       168m   coredns      k8s.gcr.io/coredns:1.6.5   k8s-app=kube-dns,pod-template-hash=6955765f44

```

这里主要是为了兼容 K8S 原有的 `kube-dns` 所以标签和 `Service` 的名字都还使用了 `kube-dns`，但实际在运行的则是 CoreDNS。

### 实践

下载个有dig命令的alpine 镜像：docker pull makocchi/alpine-dig

```  sh
# 临时开启一个pod:
# kubectl run alpine -it --rm --restart='Never' --image='makocchi/alpine-dig' sh
/ 
```



### loop 插件

https://github.com/coredns/coredns/blob/master/plugin/loop/README.md

一些讨论：

https://github.com/coredns/coredns/issues/1647

### forward 插件

https://coredns.io/plugins/forward/



## ingress

 `Service` 的 4 种基础类型，在前面的介绍中，我们一般都在使用 `ClusterIP`或 `NodePort` 等方式将服务暴露在集群内或者集群外。

介绍另一种处理服务访问的方式 `Ingress`。

**`Ingress` 是一组允许外部请求进入集群的路由规则的集合。它可以给 `Service` 提供集群外部访问的 URL，负载均衡，SSL 终止等。**

**直白点说，`Ingress` 就类似起到了智能路由的角色，外部流量到达 `Ingress` ，再由它按已经制定好的规则分发到不同的后端服务中去。**

看起来它很像我们使用的负载均衡器之类的。那你可能会问，`Ingress` 与 `LoadBalancer` 类型的 `Service` 的区别是什么呢？

- `Ingress` 不是一种 `Service` 类型

  **`Ingress` 是 K8S 中的一种资源类型**，我们可以直接通过 `kubectl get ingress` 的方式获取我们已有的 `Ingress` 资源。通过 `kubectl explain ingress` 命令，看下对 Ingress 的描述。

- `Ingress` 可以有多种控制器（实现）

  通过之前的介绍，我们知道 K8S 中有很多的 `Controller` (控制器)，而这些 `Controller` 已经打包进了 `kube-controller-manager` 中，通过 `--controllers` 参数控制启用哪些。

  但是 `Ingress` 的 `Controller` 并没有包含在其中，而且有多种选择。



## 调试k8s的DNS

起一个可用dns命令的pod:

``` yaml
apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: gcr.io/kubernetes-e2e-test-images/dnsutils:1.3
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
```

或者使用：kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml

状态：

``` sh
kubectl get pods dnsutils
NAME      READY     STATUS    RESTARTS   AGE
dnsutils   1/1       Running   0          <some-time>
```

运行nslookup命令：

``` sh
kubectl exec -ti dnsutils -- nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      kubernetes.default
Address 1: 10.0.0.1
```



### nslookup 运行出错

可能会出现：

```sh
/ # nslookup kubernetes.default
;; connection timed out; no servers could be reached

# 或
kubectl exec -ti dnsutils -- nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'kubernetes.default'

# 或
kubectl exec -ti dnsutils -- nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'kubernetes.default'
```

看一下 resolv.conf 文件：

```shell
kubectl exec -ti dnsutils -- cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local 
nameserver 10.0.0.10
options ndots:5
```

确认搜索路径和服务名，不同的云服务商可能不一样

确认DNS pod 运行状态是running:

### DNS Pod 运行状态

CoreDNS:

``` sh
kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
NAME                       READY     STATUS    RESTARTS   AGE
...
coredns-7b96bf9f76-5hsxb   1/1       Running   0           1h
coredns-7b96bf9f76-mvmmt   1/1       Running   0           1h
...
```

Kube-dns:

```sh
kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
NAME                    READY     STATUS    RESTARTS   AGE
...
kube-dns-v19-ezo1y      3/3       Running   0           1h
...
```



### DNS log

``` sh
 for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done
.:53
[INFO] plugin/reload: Running configuration MD5 = 4e235fcc3696966e76816bcd9034ebc7
CoreDNS-1.6.5
linux/amd64, go1.13.4, c2fd1b2
[ERROR] plugin/errors: 2 6936741458835321801.4609081455828696090. HINFO: read udp 10.244.0.3:54867->192.168.59.241:53: i/o timeout
[ERROR] plugin/errors: 2 6936741458835321801.4609081455828696090. HINFO: read udp 10.244.0.3:48954->192.168.59.241:53: i/o timeout
...: read udp 10.244.0.3:36185->192.168.59.241:53: i/o timeout
[ERROR] plugin/errors: 2 6936741458835321801.4609081455828696090. HINFO: read udp 10.244.0.3:38958->192.168.59.241:53: i/o timeout
.:53
[INFO] plugin/reload: Running configuration MD5 = 4e235fcc3696966e76816bcd9034ebc7
CoreDNS-1.6.5
linux/amd64, go1.13.4, c2fd1b2
[ERROR] plugin/errors: 2 6074140797085019225.7868176384750749927. HINFO: read udp 10.244.0.2:34279->192.168.59.241:53: i/o timeout
[ERROR] plugin/errors: 2 6074140797085019225.7868176384750749927. HINFO: read udp 10.244.0.2:57237->192.168.59.241:53: i/o timeout
[ERROR] plugin/errors: 2 6074140797085019225.7868176384750749927. HINFO: ...
[ERROR] plugin/errors: 2 6074140797085019225.7868176384750749927. HINFO: read udp 10.244.0.2:58751->192.168.59.241:53: i/o timeout
```

这里为什么会使用主机的dns设置, 因为coredns默认会读取主机的dns配置：

/etc/systemd/resolved.conf:

```sh
[Resolve]
DNS=192.168.59.241
#Domains=
#LLMNR=no
#MulticastDNS=no
#DNSSEC=no
#Cache=yes
#DNSStubListener=yes
```

可以参考：

https://segmentfault.com/a/1190000015639327

https://github.com/easzlab/kubeasz/issues/423 

https://github.com/coredns/coredns/issues/2087



去掉 loop 后又出现新的问题：

###  dial tcp 10.96.0.1:443: i/o timeout

可能是之前的cni0和现在用的子网范围不一致导致的：

```sh
root@node200:~# ip r
10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 linkdown

# kubeadm init --pod-network-cidr=10.244.0.0/16
```

删除 cn0,重新reset 即可：

``` sh
ip link delete cni0
```





### DNS service 

```shell
kubectl get svc --namespace=kube-system
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
...
kube-dns     ClusterIP   10.0.0.10      <none>        53/UDP,53/TCP        1h
...
```

endpoints暴露了么？

 `kubectl get endpoints` command：

```shell
kubectl get ep kube-dns --namespace=kube-system
NAME       ENDPOINTS                       AGE
kube-dns   10.180.3.17:53,10.180.3.17:53    1h
```





### 添加log输出

在coredns添加log插件：

``` sh 
kubectl -n kube-system edit configmap coredns

apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        log   # ！
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          upstream
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }

```

